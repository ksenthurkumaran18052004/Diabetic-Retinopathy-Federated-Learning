{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":988278,"sourceType":"datasetVersion","datasetId":541202}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AGAIN 2","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport tensorflow as tf\n\n\n# Step 1: Split dataset into 60% train and 40% test\ndef split_data_60_40(image_dir):\n    all_images = []\n    all_labels = []\n\n    for folder_name in os.listdir(image_dir):\n        folder_path = os.path.join(image_dir, folder_name)\n        if os.path.isdir(folder_path):\n            for img_name in os.listdir(folder_path):\n                if img_name.lower().endswith(('.jpeg', '.jpg', '.png')):\n                    all_images.append(os.path.join(folder_path, img_name))\n                    all_labels.append(folder_name)\n\n    print(f\"Total images found: {len(all_images)}\")\n    print(f\"Classes found: {set(all_labels)}\")\n\n    if not all_images:\n        raise ValueError(f\"No images found in the directory: {image_dir}. Check the directory structure.\")\n\n    # Split data into 60% train and 40% test\n    train_images, test_images, train_labels, test_labels = train_test_split(\n        all_images, all_labels, test_size=0.4, stratify=all_labels, random_state=42\n    )\n    return train_images, test_images, train_labels, test_labels\n\n\n# Step 2: Preprocess images\ndef preprocess_images(image_paths, labels, class_indices, target_size=(224, 224)):\n    X, y = [], []\n    for img_path, label in zip(image_paths, labels):\n        try:\n            img = load_img(img_path, target_size=target_size)\n            img_array = img_to_array(img) / 255.0\n            X.append(img_array)\n            y.append(class_indices[label])\n        except Exception as e:\n            print(f\"Error processing image {img_path}: {e}\")\n\n    X = np.array(X)\n    y = to_categorical(y, num_classes=len(class_indices))\n    return X, y\n\n\n# Step 3: Create a fine-tuned MobileNetV2 model with weighted loss\ndef create_finetuned_model(class_weights):\n    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n    base_model.trainable = True  # Allow fine-tuning\n\n    for layer in base_model.layers[:50]:  # Freeze fewer layers\n        layer.trainable = False\n\n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.Dropout(0.3)(x)\n    output = layers.Dense(5, activation='softmax')(x)\n\n    model = models.Model(inputs=base_model.input, outputs=output)\n\n    # Custom weighted loss\n    def weighted_categorical_crossentropy(y_true, y_pred):\n        weights = tf.reduce_sum(class_weights * y_true, axis=-1)\n        loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n        return loss * weights\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=weighted_categorical_crossentropy, metrics=['accuracy'])\n    return model\n\n\n# Step 4: Federated Learning with Data Generators\ndef federated_learning_with_generators(clients_data, class_indices, class_weights, epochs=10, batch_size=16, target_size=(224, 224)):\n    \"\"\"\n    Federated Learning implementation using data generators and weighted loss.\n    \"\"\"\n    global_model = create_finetuned_model(class_weights)\n    global_weights = global_model.get_weights()\n\n    for round in range(epochs):\n        print(f\"Round {round + 1} of federated learning...\")\n\n        client_weights = []\n        for client_id, client_images in enumerate(clients_data):\n            print(f\"Processing client {client_id + 1} with {len(client_images)} images...\")\n\n            # Create a generator for the client's data\n            def client_generator(client_images, batch_size=batch_size):\n                while True:\n                    for i in range(0, len(client_images), batch_size):\n                        batch_images = client_images[i:i + batch_size]\n                        X, y = preprocess_images(\n                            batch_images,\n                            [os.path.basename(os.path.dirname(img)) for img in batch_images],\n                            class_indices,\n                            target_size=target_size\n                        )\n                        yield X, y\n\n            # Train client model using generator\n            client_model = create_finetuned_model(class_weights)\n            client_model.set_weights(global_weights)\n\n            steps_per_epoch = len(client_images) // batch_size\n            generator = client_generator(client_images)\n            client_model.fit(generator, steps_per_epoch=steps_per_epoch, epochs=1, verbose=1)\n\n            client_weights.append(client_model.get_weights())\n\n        # Aggregate client weights\n        if client_weights:\n            global_weights = [np.mean([client_w[layer] for client_w in client_weights], axis=0)\n                              for layer in range(len(global_weights))]\n        else:\n            print(\"No valid client weights collected. Global weights remain unchanged.\")\n\n    global_model.set_weights(global_weights)\n    return global_model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T14:45:30.020292Z","iopub.execute_input":"2025-01-20T14:45:30.020553Z","iopub.status.idle":"2025-01-20T14:45:30.036772Z","shell.execute_reply.started":"2025-01-20T14:45:30.020529Z","shell.execute_reply":"2025-01-20T14:45:30.035757Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Step 5: Efficient Evaluation of the Global Model\ndef evaluate_global_model(global_model, test_images, test_labels, class_indices, batch_size=16, target_size=(224, 224)):\n    \"\"\"\n    Evaluate the global model efficiently by processing predictions in batches.\n    \"\"\"\n    y_true = []\n    y_pred = []\n\n    for i in range(0, len(test_images), batch_size):\n        # Batch the test images and labels\n        batch_images = test_images[i:i + batch_size]\n        batch_labels = test_labels[i:i + batch_size]\n        X, y = preprocess_images(batch_images, batch_labels, class_indices, target_size=target_size)\n        y_true.extend(np.argmax(y, axis=1))  # True labels\n        y_pred.extend(np.argmax(global_model.predict(X, verbose=0), axis=1))  # Predictions\n\n    # Compute metrics\n    target_names = list(class_indices.keys())\n    report = classification_report(y_true, y_pred, target_names=target_names)\n    confusion = confusion_matrix(y_true, y_pred)\n\n    # Output results\n    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n    print(f\"Test Accuracy: {accuracy:.4f}\")\n    print(\"\\nClassification Report:\\n\", report)\n    print(\"\\nConfusion Matrix:\\n\", confusion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T14:45:30.038480Z","iopub.execute_input":"2025-01-20T14:45:30.038799Z","iopub.status.idle":"2025-01-20T14:45:30.054975Z","shell.execute_reply.started":"2025-01-20T14:45:30.038769Z","shell.execute_reply":"2025-01-20T14:45:30.054279Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Main Execution\nif __name__ == \"__main__\":\n    # Dataset path\n    image_dir = \"/kaggle/input/diabetic-retinopathy-2015-data-colored-resized/colored_images/colored_images\"\n\n    # Split data into 60% train and 40% test\n    train_images, test_images, train_labels, test_labels = split_data_60_40(image_dir)\n    print(f\"Training images: {len(train_images)}, Testing images: {len(test_images)}\")\n\n    # Set up data generator to get class indices\n    datagen = ImageDataGenerator(\n        rescale=1.0 / 255,\n        rotation_range=30,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        validation_split=0.2\n    )\n    train_generator = datagen.flow_from_directory(\n        directory=image_dir,\n        target_size=(224, 224),\n        batch_size=16,\n        class_mode='categorical',\n        subset='training'\n    )\n    class_indices = train_generator.class_indices\n\n    # Calculate class weights\n    class_weights = compute_class_weight(\n        'balanced', classes=np.unique(train_labels), y=train_labels\n    )\n    class_weights_array = np.array([class_weights[i] for i in range(len(class_weights))], dtype=np.float32)\n\n    # Divide training data into federated clients\n    num_clients = 4\n    client_data_size = len(train_images) // num_clients\n    clients_data = [train_images[i * client_data_size:(i + 1) * client_data_size] for i in range(num_clients)]\n\n    # Federated learning with generators and weighted loss (10 Rounds)\n    global_model = federated_learning_with_generators(clients_data, class_indices, class_weights_array, epochs=3, batch_size=16)\n\n    # Efficient evaluation of the global model\n    evaluate_global_model(global_model, test_images, test_labels, class_indices, batch_size=16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T14:46:10.203079Z","iopub.execute_input":"2025-01-20T14:46:10.203471Z","iopub.status.idle":"2025-01-20T14:56:51.741367Z","shell.execute_reply.started":"2025-01-20T14:46:10.203441Z","shell.execute_reply":"2025-01-20T14:56:51.740208Z"}},"outputs":[{"name":"stdout","text":"Total images found: 35126\nClasses found: {'No_DR', 'Proliferate_DR', 'Moderate', 'Severe', 'Mild'}\nTraining images: 21075, Testing images: 14051\nFound 28103 images belonging to 5 classes.\nRound 1 of federated learning...\nProcessing client 1 with 5268 images...\n\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 42ms/step - accuracy: 0.2836 - loss: 1.6188\nProcessing client 2 with 5268 images...\n\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - accuracy: 0.3126 - loss: 1.5277\nProcessing client 3 with 5268 images...\n\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 41ms/step - accuracy: 0.3421 - loss: 1.4976\nProcessing client 4 with 5268 images...\n\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - accuracy: 0.3241 - loss: 1.6785\nRound 2 of federated learning...\nProcessing client 1 with 5268 images...\n\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - accuracy: 0.4164 - loss: 1.2887\nProcessing client 2 with 5268 images...\n\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 45ms/step - accuracy: 0.4096 - loss: 1.2263\nProcessing client 3 with 5268 images...\n\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 42ms/step - accuracy: 0.4387 - loss: 1.1842\nProcessing client 4 with 5268 images...\n\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 42ms/step - accuracy: 0.4393 - loss: 1.2991\nRound 3 of federated learning...\nProcessing client 1 with 5268 images...\n\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - accuracy: 0.4599 - loss: 1.1001\nProcessing client 2 with 5268 images...\n\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 41ms/step - accuracy: 0.4607 - loss: 1.0346\nProcessing client 3 with 5268 images...\n\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 46ms/step - accuracy: 0.5103 - loss: 1.0079\nProcessing client 4 with 5268 images...\n\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - accuracy: 0.5015 - loss: 1.1269\nTest Accuracy: 0.6868\n\nClassification Report:\n                 precision    recall  f1-score   support\n\n          Mild       0.08      0.11      0.09       977\n      Moderate       0.25      0.01      0.03      2117\n         No_DR       0.75      0.92      0.83     10325\nProliferate_DR       0.60      0.06      0.12       283\n        Severe       0.57      0.01      0.02       349\n\n      accuracy                           0.69     14051\n     macro avg       0.45      0.22      0.22     14051\n  weighted avg       0.62      0.69      0.62     14051\n\n\nConfusion Matrix:\n [[ 105    2  869    0    1]\n [ 262   31 1821    2    1]\n [ 802   24 9492    7    0]\n [  37   34  193   18    1]\n [  70   33  239    3    4]]\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}